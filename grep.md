# Notes on grep

Command line can be particularly useful for searching through multiple and/or large documents.

The `grep` command in particular allows you to search for patterns and grab the documents or sentences containing them.

## An example of using GREP to extract links in one webpage

[This webpage](https://avinor.no/konsern/om-oss/trafikkstatistikk/trafikkstatistikk) is one example of where this can be useful. It contains links to a number of Excel spreadsheets, but the links are generated by JavaScript, so cannot be scraped using Google Sheets functions like IMPORTHTML or IMPORTXML.

You can instead, however, download the webpage and use grep to find the links to the spreadsheets buried in that JavaScript.

I've [copied the JSON in the source HTML into a text file here](https://github.com/paulbradshaw/commandline/blob/master/TRAFIKKSTATISTIKK.txt)

In Terminal, navigate to the folder containing that file and type the following command:

`grep "[0-9a-zA-Z_-]*\.xlsx" TRAFIKKSTATISTIKK.txt -o`

The bit after `grep` is **regex**: square brackets indicate that you want to look for any of the characters within: `0-9` means any digit; `a-z` means any lower case character; `A-Z` any upper case character, and `_` and `-` are included because some file names contain those as well.

The `*` means *none or more* of those characters, followed by...

`\.`, which means a full stop. The backslash is used to indicate that the period `.` should be taken literally - in regex the period has a special meaning otherwise (any character).

...followed by `xlsx`

So this is looking for a series of any of the specified characters, followed by `.xlsx`. This will find our file names.

That regex is followed by the name of the file(s) to search, and then `-o`

`-o` means we want to bring back the match *only*. If we don't include that, it will bring back any lines containing a match, but as our file is all one single line, that would mean the whole line.

The series of characters does not include `/` so that is going to be the starting point *before* our matches.

Results can then be saved as new file.

## An example of using GREP on multiple files

In this example I'm using the Lords Register of Interests - elsewhere in this repo [I explain how to download those pages using `curl`](https://github.com/paulbradshaw/commandline/blob/master/curlscraping.md), but you can also do this manually. Either way you'll need a few HTML pages first, and to navigate into the folder containing those.

If we wanted the names, the HTML before the information we want would look like this (note the space):

`Lord `

And the HTML after is:

`</td></tr><tr><td class="lordsinterestcategory">`

So we might write:

`grep "Lord .*</td></tr><tr><td class=" [A-E].html -o > results.txt`

Other options include:

`grep "Police and Crime Commissioner" [A-E].html -o > results.txt`

`grep "Chairman, [A-Za-z]*" [A-E].html -o > results.txt`

`grep "Chairman, \w*\s\w*\s\w*\s\w*\s\w*" [A-E].html -o > results.txt`
